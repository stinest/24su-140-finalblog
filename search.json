[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "24su-140-finalblog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Climate Talk\n\n\n\n\n\nDH 140 Final Project\n\n\n\n\n\nAug 1, 2024\n\n\nJustine Lim\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJul 29, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/DH140_JL_Final.html",
    "href": "posts/DH140_JL_Final.html",
    "title": "Analyzing Climate Talk",
    "section": "",
    "text": "In the complex fight against climate change, effective communication is key. Understanding how different rhetoric is used to shape public perception and inspire action can make all the difference. I want to find which types of language are most indicative of propaganda versus scientifically backed information to gauge their influence on public understanding of climate change.\nFrom understanding the ways propaganda and scientifically backed works communicate their messages, we can learn how to better inform and engage the public. Analyzing the language used in these different types of rhetoric will help us understand the divides that influence how climate change is perceived and acted upon. My research intends to break these divisions and instead promote a more informed and unified approach to understanding environmental issues.\nBy not only addressing an urgent global challenge but also using the power of digital humanities to find the intricate ways in which language shapes our world, my study aims to contribute to more effective climate communication strategies for greater public awareness and action.\n\n\n\n\nIn what ways have rhetorical strategies been used in written works about climate change, and what can exploratory data analysis of sentimental language reveal about the presence of propaganda versus scientifically backed information in reputable publications?\n\nThe primary motivation for pursuing this research question came from completing Assignment 05, where we learned how to use Natural Language Processing to create strong data visualizations. I wanted to apply these tools to a new dataset on a topic I feel deeply about. I believe that understanding the severity of global warming is crucial, and one of the issues is how we discuss it. The language we use often fails to convey the urgency of the situation, and I want to explore how this affects public perception and potential solutions.\nAll my data will be compiled from the online Climate section of The Atlantic, because after testing many reputable publications, I found that it has fewer restrictions on web scraping. I will use Python packages specializing in web scraping, such as Beautiful Soup and Requests, to parse HTML documents and extract links to various articles. Once I have access to these articles, I will use Pandas to organize the data into an analyzable format. For text processing, I will utilize NLTK, and for creating visualizations, I will mainly use Matplotlib.\n\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nThe first step in our analysis was to set up the web scraping process. To retrieve data from the web, I imported two libraries, requests to retrieve the HTML code from The Atlantic’s Climate webpage and BeautifulSoup to parse the HTML and find links to various articles on the page.\n\nresponse = requests.get('https://www.theatlantic.com/category/climate/')\nhtml_string = response.text\n\ndoc = BeautifulSoup(html_string, 'html.parser')\narticle_list = []\n    \nfor li in doc.select('ul li a'):\n        href = li.get('href')\n        if href and '/archive' in href:\n            article_list.append(href)\n          \nfor a in doc.find_all('a', href=True):\n    href = a['href']\n    if '/archive' in href:\n        article_list.append(href)\n\nBy searching for all occurrences of anchor tags within lists and also all anchor tags on the page, I created a variable article_list that contains a list of links to the latest Atlantic articles on climate. To avoid irrelevant links to images and other navigation, I filtered specifically for links that contain /archive in their href attribute.\n\ndef getArticleDetails(article):\n  response = requests.get(article)\n  doc = BeautifulSoup(response.text, 'html.parser')\n  \n  title = doc.find('h1')\n  article_title = title.get_text()\n  \n  article_body = doc.find_all(class_='ArticleParagraph_root__4mszW')\n  article_text = ''\n  \n  for paragraph in article_body:\n    article_text += paragraph.get_text() + '\\n'\n    \n  return article_title, article_text\n\nI have defined a function getArticleDetails to open a particular url and return its title and paragraph text in a string form without unnecessary headers or links.\n\nimport random\n\nsampled_articles = random.sample(article_list, 6)\n\nBecause The Atlantic has an abundance of written works, I imported Python’s random module to take a random sample of 6 article URLs from the list to control my data analysis.\n\nimport pandas as pd\n\narticleDF = pd.DataFrame(columns=['url', 'text'])\n\ndf_list = []\n\nfor article_url in sampled_articles:\n  if article_url.startswith('/'):\n    article_url = 'https://www.theatlantic.com' + article_url\n  article_title, article_text = getArticleDetails(article_url)\n    \n  word_count = len(article_text.split())\n  \n  df_list.append(pd.DataFrame({'url': [article_url], 'title': [article_title], 'text': [article_text], 'word_count': [word_count]}))\n\narticleDF = pd.concat(df_list, ignore_index=True)\n\nI have now imported the pandas library to set up for data manipulation and analysis. I then set up a for-loop that retrieved the title and text using the getArticleDetails function defined earlier. I also used the len() tool to calculate each article’s word count. To organize all the data and prevent overwriting variables with each iteration, I created a list to store individual data frames for each article’s article_url, article_title, article_text, and word_count. After collecting all the data frames, I concatenated them into a single data frame to store the information for all articles in the sample.\n\n\n\n\nprint(articleDF)\n\n                                                 url  \\\n0  https://www.theatlantic.com/science/archive/20...   \n1  https://www.theatlantic.com/science/archive/20...   \n2  https://www.theatlantic.com/science/archive/20...   \n3  https://www.theatlantic.com/science/archive/20...   \n4  https://www.theatlantic.com/magazine/archive/2...   \n5  https://www.theatlantic.com/technology/archive...   \n\n                                               title  \\\n0            Plastic Has Changed Sea Turtles Forever   \n1  Do Not Allow Even One Fruit Fly Into Your Kitc...   \n2  One Huge Contradiction Is Undoing Our Best Cli...   \n3            Plastic Has Changed Sea Turtles Forever   \n4  The Climate Can’t Afford Another Trump Presidency   \n5         Texas Is a Look Into the Future of Driving   \n\n                                                text  word_count  \n0  As recently as the 1960s, perhaps later—within...        1109  \n1  Last week, as my spouse and I were settling in...        1097  \n2  You’d be forgiven for thinking that the fight ...        1652  \n3  As recently as the 1960s, perhaps later—within...        1109  \n4  On the last Saturday before Donald Trump took ...         768  \n5  Every Texan I know has what you might call “gr...        1421  \n\n\nIn the above code, I have printed out the main dataframe we created to summarize the data we will be using for our analysis. As we can see, there are three main columns with 6 rows for each of the 6 articles we will be comparing. The columns store its url, title, text, and total word count.\n\nimport matplotlib.pyplot as plt\n\n\nplt.scatter(articleDF['title'], articleDF['word_count'])\n\n\n\n\n\n\n\n\nAfter importing the matplotlib.pyplot package, I was well equipped to begin creating exploratory visualizations. To make one that compared articles with their word count, I chose to do a scatterplot, but ran into a few obvious issues:\n\nThe titles, being too long, overlapped over one another.\nThe points were all the same color, making it difficult to distinguish from one another.\n\n\nplt.plot(articleDF['title'], articleDF['word_count'], color='black', linewidth=0.5)\nplt.scatter(articleDF['title'], articleDF['word_count'], c=range(len(articleDF)))\n\nplt.xlabel('Article Title')\nplt.ylabel('Word Count')\nplt.title('Word Count of Various Climate Articles from The Atlantic')\n\nplt.xticks(rotation=45, ha='right')\n\n([0, 1, 2, 3, 4],\n [Text(0, 0, 'Plastic Has Changed Sea Turtles Forever'),\n  Text(1, 0, 'Do Not Allow Even One Fruit Fly Into Your Kitchen Compost'),\n  Text(2, 0, 'One Huge Contradiction Is Undoing Our Best Climate Efforts'),\n  Text(3, 0, 'The Climate Can’t Afford Another Trump Presidency'),\n  Text(4, 0, 'Texas Is a Look Into the Future of Driving')])\n\n\n\n\n\n\n\n\n\nBy adding a color setting within the plt.scatter() function, I was able to create distinctive colors per point, and by rotating the x-axis labels 45 degrees to the right, each title was able to be read by itself without any overlap. After I added a title and labels to the x-axis and y-axis, to make the visual even more clear, I overlapped the scatterplot with a lineplot to accentuate the trend.\nHowever, even with these adjustments, I was still not satisfied with the visualization, and felt that there was a better plot I could choose.\n\nplt.bar(articleDF['title'], articleDF['word_count'])\n\nplt.xlabel('Article Title')\nplt.ylabel('Word Count')\nplt.title('Word Count of Various Climate Articles from The Atlantic')\n\nplt.xticks(rotation=45, ha='right')\n\n([0, 1, 2, 3, 4],\n [Text(0, 0, 'Plastic Has Changed Sea Turtles Forever'),\n  Text(1, 0, 'Do Not Allow Even One Fruit Fly Into Your Kitchen Compost'),\n  Text(2, 0, 'One Huge Contradiction Is Undoing Our Best Climate Efforts'),\n  Text(3, 0, 'The Climate Can’t Afford Another Trump Presidency'),\n  Text(4, 0, 'Texas Is a Look Into the Future of Driving')])\n\n\n\n\n\n\n\n\n\nAfter using those same adjustments on a verticle bar plot, I think this visualization best shows the trend in word count in an understandable and simple way, and gives me a good foundation for the next step in my project.\n\n\n\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment import vader\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import opinion_lexicon\nfrom nltk.stem.porter import PorterStemmer\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('vader_lexicon')\nnltk.download('opinion_lexicon')\n\n[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package opinion_lexicon to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Package opinion_lexicon is already up-to-date!\n\n\nTrue\n\n\nTo begin the data analysis portion of my final project, I have imported the Natural Language Toolkit (NLTK) and its various modules to help with starting a more focused analysis.\n\nstop_words = (stopwords.words('english'))\nsenti = vader.SentimentIntensityAnalyzer()\n\ndef processText(text):\n  tokenized_text = word_tokenize(text)\n  noStop_text = [word for word in tokenized_text if word.lower() not in stop_words]\n  \n  stemmed_text = [PorterStemmer().stem(word) for word in noStop_text]\n  return stemmed_text\n\narticleDF['processed'] = articleDF['text'].apply(processText)\n\nAbove, I began by creating two variables to help me with syntax in using NLTK modules. I then defined a function to process the raw text from The Atlantic articles by tokenizing the string, removing stop words, and words with the same stem.\n\n\n\n\n# compares total frequency of positive and negative words\ndef PosNeg(text):\n  pos_count = 0\n  neg_count = 0\n\n  for word in text:\n    score = senti.polarity_scores(word)\n    if score['compound'] &gt; 0:\n      pos_count += 1\n    elif score['compound'] &lt; 0:\n      neg_count += 1\n\n  return pos_count, neg_count\n\n\nfig, axis = plt.subplots(2, 3, figsize=(18, 12))  # fits all plots in one page\naxis = axis.flatten()\n\nfor i in range(len(articleDF)):   # for each row/article in articleDF\n    row = articleDF.iloc[i]\n\n    pos_count, neg_count = PosNeg(row['processed'])\n\n    axis[i].bar('Positive Words', pos_count, color='yellowgreen', label='Positive Words')\n    axis[i].bar('Negative Words', neg_count, color='firebrick', label='Negative Words')\n\n    axis[i].set_title(row['title'])\n    axis[i].set_xlabel('Word Type')\n    axis[i].set_ylabel('Frequency')\n    axis[i].legend()\n    \nplt.tight_layout()  # no axis overlap\n\n\n\n\n\n\n\n\nThe PosNeg() function takes in a given text and calculates the total number of positive and negative words by using the VADER sentiment analyzer to score each word, incrementing variables pos_count for positive scores and neg_count for negative scores. The function returns the counts of positive and negative words, visualizing information into the general sentiment of the text.\n\n# compares 15 most common positive and negative words\ndef PosNegFreq(text):\n  pos_words = []\n  neg_words = []\n  \n  for word in text:\n    score = senti.polarity_scores(word)\n    if score['compound'] &gt; 0:\n      pos_words.append(word)\n    elif score['compound'] &lt; 0:\n      neg_words.append(word)\n\n  pos_common = nltk.FreqDist(pos_words).most_common(15)\n  neg_common = nltk.FreqDist(neg_words).most_common(15)\n  \n  pos = []\n  pos_freq = []\n  for i in pos_common:\n    pos.append(i[0])\n    pos_freq.append(i[1])\n    \n  neg = []\n  neg_freq = []\n  for i in neg_common:\n    neg.append(i[0])\n    neg_freq.append(i[1])\n    \n  return pos, pos_freq, neg, neg_freq\n\n\nfig, axis = plt.subplots(2, 3, figsize=(18, 12))  # fits all plots in one page\naxis = axis.flatten()\n\nfor i in range(len(articleDF)):   # for each row/article in articleDF\n    row = articleDF.iloc[i]\n\n    pos, pos_freq, neg, neg_freq = PosNegFreq(row['processed'])\n\n    axis[i].barh(neg, neg_freq, color='firebrick', label='Negative Words')\n    axis[i].barh(pos, pos_freq, color='yellowgreen', label='Positive Words')\n\n    axis[i].set_title(row['title'])\n    axis[i].set_xlabel('Frequency')\n    axis[i].set_ylabel('Words')\n    axis[i].legend()\n    \nplt.tight_layout()  # no axis overlap\n\n\n\n\n\n\n\n\nThe PosNegFreq() function identifies the 15 most common positive and negative words in a given text. It classifies words based on their sentiment scores using the VADER analyzer, and then calculates their frequencies, returning four lists: the most common positive words, their frequencies, the most common negative words, and their frequencies.\n\ndef RankByNeg(text):\n  neg_score = []\n  \n  for index, row in text.iterrows():\n    pos_count, neg_count = PosNeg(row['processed'])\n    neg_score.append((row['title'], neg_count))\n    \n  for i in range(len(neg_score)):\n    for j in range(i + 1, len(neg_score)):\n      if neg_score[i][1] &lt; neg_score[j][1]:\n        neg_score[i], neg_score[j] = neg_score[j], neg_score[i]\n  \n  return neg_score\n\n\nneg_scores = RankByNeg(articleDF)\n\ntitles = [item[0] for item in neg_scores]\nscores = [item[1] for item in neg_scores]\n\nplt.barh(titles, scores, color='firebrick')\nplt.xlabel('Negativity Score')\nplt.ylabel('Article Title')\nplt.title('Negative Word Count by Article')\n\nText(0.5, 1.0, 'Negative Word Count by Article')\n\n\n\n\n\n\n\n\n\nThe RankByNeg() function calculates negativity scores (total number of negative words) for each article using the previous PosNeg() function and sorts the list of scores in order.\n\ndef RankByPos(text):\n  pos_score = []\n  \n  for index, row in text.iterrows():\n    pos_count, neg_count = PosNeg(row['processed'])\n    pos_score.append((row['title'], pos_count))\n    \n  for i in range(len(pos_score)):\n    for j in range(i + 1, len(pos_score)):\n      if pos_score[i][1] &lt; pos_score[j][1]:\n        pos_score[i], pos_score[j] = pos_score[j], pos_score[i]\n  \n  return pos_score\n\n\npos_scores = RankByPos(articleDF)\n\ntitles = [item[0] for item in pos_scores]\nscores = [item[1] for item in pos_scores]\n\nplt.barh(titles, scores, color='yellowgreen')\nplt.xlabel('Positivity Score')\nplt.ylabel('Article Title')\nplt.title('Positive Word Count by Article')\n\nText(0.5, 1.0, 'Positive Word Count by Article')\n\n\n\n\n\n\n\n\n\nThe RankByPos() function calculates positivity scores (total number of positive words) for each article using the previous PosNeg() function and sorts the list of scores in order.\n\n\n\nMy preliminary analysis has given me valuable information on how rhetorical strategies are used in written works about climate change. By examining the frequency and sentiment of specific word choices, I have began to uncover patterns that reveal how language is used to frame climate change issues in various articles.\nThe visualizations generated from the data reveal a sharp divide in language use between articles that focus on raising awareness about climate change and those proposing solutions. The article We’re in an Age of Fire exhibits a higher frequency of negative language, with terms such as “fire” and “problem” predominating. This suggests that these articles are more likely to emphasize the urgency and severity of climate change, potentially to provoke concern and action.\nOn the other hand, articles that propose solutions or discuss positive developments, such as A Radical Idea to Break the Logic of Oil Drilling, tend to use more positive language. Words like “warm” and “better”, with higher positive scores, are more frequently used, showing a focus on hopeful narratives and potential advancements. This difference in language use is an example of a rhetorical strategy where negative language is used to show the critical nature of the problem, while positive language is used to convey optimism and the possibility for progress.\nIn regards to my research focus, propaganda typically relies on emotionally charged negative language, while scientific information balance both positive and negative language. With a more robust analysis involving multiple sources and a larger dataset, I could fully answer my research question and differentiate between the two.\n\n\n\nThis research shows the important role of how we talk about climate change. The words and phrases used in climate change articles can strongly influence how people understand and respond to the issue. When the language used shows urgency and the seriousness of the problem, it can encourage people to take action and better understand the crisis. However, if the language downplays the severity or misrepresents the facts, it can slow down efforts to address climate change. Thus, the way climate change is talked about in public can either help or hinder the efforts to deal with this global problem.\nOur findings provide useful information for creating better ways to communicate about climate change, guiding media practices, and supporting efforts to encourage more action and possible policy changes."
  },
  {
    "objectID": "posts/DH140_JL_Final.html#dh140-final-project-by-justine-lim",
    "href": "posts/DH140_JL_Final.html#dh140-final-project-by-justine-lim",
    "title": "Analyzing Climate Talk",
    "section": "",
    "text": "In the complex fight against climate change, effective communication is key. Understanding how different rhetoric is used to shape public perception and inspire action can make all the difference. I want to find which types of language are most indicative of propaganda versus scientifically backed information to gauge their influence on public understanding of climate change.\nFrom understanding the ways propaganda and scientifically backed works communicate their messages, we can learn how to better inform and engage the public. Analyzing the language used in these different types of rhetoric will help us understand the divides that influence how climate change is perceived and acted upon. My research intends to break these divisions and instead promote a more informed and unified approach to understanding environmental issues.\nBy not only addressing an urgent global challenge but also using the power of digital humanities to find the intricate ways in which language shapes our world, my study aims to contribute to more effective climate communication strategies for greater public awareness and action.\n\n\n\n\nIn what ways have rhetorical strategies been used in written works about climate change, and what can exploratory data analysis of sentimental language reveal about the presence of propaganda versus scientifically backed information in reputable publications?\n\nThe primary motivation for pursuing this research question came from completing Assignment 05, where we learned how to use Natural Language Processing to create strong data visualizations. I wanted to apply these tools to a new dataset on a topic I feel deeply about. I believe that understanding the severity of global warming is crucial, and one of the issues is how we discuss it. The language we use often fails to convey the urgency of the situation, and I want to explore how this affects public perception and potential solutions.\nAll my data will be compiled from the online Climate section of The Atlantic, because after testing many reputable publications, I found that it has fewer restrictions on web scraping. I will use Python packages specializing in web scraping, such as Beautiful Soup and Requests, to parse HTML documents and extract links to various articles. Once I have access to these articles, I will use Pandas to organize the data into an analyzable format. For text processing, I will utilize NLTK, and for creating visualizations, I will mainly use Matplotlib.\n\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nThe first step in our analysis was to set up the web scraping process. To retrieve data from the web, I imported two libraries, requests to retrieve the HTML code from The Atlantic’s Climate webpage and BeautifulSoup to parse the HTML and find links to various articles on the page.\n\nresponse = requests.get('https://www.theatlantic.com/category/climate/')\nhtml_string = response.text\n\ndoc = BeautifulSoup(html_string, 'html.parser')\narticle_list = []\n    \nfor li in doc.select('ul li a'):\n        href = li.get('href')\n        if href and '/archive' in href:\n            article_list.append(href)\n          \nfor a in doc.find_all('a', href=True):\n    href = a['href']\n    if '/archive' in href:\n        article_list.append(href)\n\nBy searching for all occurrences of anchor tags within lists and also all anchor tags on the page, I created a variable article_list that contains a list of links to the latest Atlantic articles on climate. To avoid irrelevant links to images and other navigation, I filtered specifically for links that contain /archive in their href attribute.\n\ndef getArticleDetails(article):\n  response = requests.get(article)\n  doc = BeautifulSoup(response.text, 'html.parser')\n  \n  title = doc.find('h1')\n  article_title = title.get_text()\n  \n  article_body = doc.find_all(class_='ArticleParagraph_root__4mszW')\n  article_text = ''\n  \n  for paragraph in article_body:\n    article_text += paragraph.get_text() + '\\n'\n    \n  return article_title, article_text\n\nI have defined a function getArticleDetails to open a particular url and return its title and paragraph text in a string form without unnecessary headers or links.\n\nimport random\n\nsampled_articles = random.sample(article_list, 6)\n\nBecause The Atlantic has an abundance of written works, I imported Python’s random module to take a random sample of 6 article URLs from the list to control my data analysis.\n\nimport pandas as pd\n\narticleDF = pd.DataFrame(columns=['url', 'text'])\n\ndf_list = []\n\nfor article_url in sampled_articles:\n  if article_url.startswith('/'):\n    article_url = 'https://www.theatlantic.com' + article_url\n  article_title, article_text = getArticleDetails(article_url)\n    \n  word_count = len(article_text.split())\n  \n  df_list.append(pd.DataFrame({'url': [article_url], 'title': [article_title], 'text': [article_text], 'word_count': [word_count]}))\n\narticleDF = pd.concat(df_list, ignore_index=True)\n\nI have now imported the pandas library to set up for data manipulation and analysis. I then set up a for-loop that retrieved the title and text using the getArticleDetails function defined earlier. I also used the len() tool to calculate each article’s word count. To organize all the data and prevent overwriting variables with each iteration, I created a list to store individual data frames for each article’s article_url, article_title, article_text, and word_count. After collecting all the data frames, I concatenated them into a single data frame to store the information for all articles in the sample.\n\n\n\n\nprint(articleDF)\n\n                                                 url  \\\n0  https://www.theatlantic.com/science/archive/20...   \n1  https://www.theatlantic.com/science/archive/20...   \n2  https://www.theatlantic.com/science/archive/20...   \n3  https://www.theatlantic.com/science/archive/20...   \n4  https://www.theatlantic.com/magazine/archive/2...   \n5  https://www.theatlantic.com/technology/archive...   \n\n                                               title  \\\n0            Plastic Has Changed Sea Turtles Forever   \n1  Do Not Allow Even One Fruit Fly Into Your Kitc...   \n2  One Huge Contradiction Is Undoing Our Best Cli...   \n3            Plastic Has Changed Sea Turtles Forever   \n4  The Climate Can’t Afford Another Trump Presidency   \n5         Texas Is a Look Into the Future of Driving   \n\n                                                text  word_count  \n0  As recently as the 1960s, perhaps later—within...        1109  \n1  Last week, as my spouse and I were settling in...        1097  \n2  You’d be forgiven for thinking that the fight ...        1652  \n3  As recently as the 1960s, perhaps later—within...        1109  \n4  On the last Saturday before Donald Trump took ...         768  \n5  Every Texan I know has what you might call “gr...        1421  \n\n\nIn the above code, I have printed out the main dataframe we created to summarize the data we will be using for our analysis. As we can see, there are three main columns with 6 rows for each of the 6 articles we will be comparing. The columns store its url, title, text, and total word count.\n\nimport matplotlib.pyplot as plt\n\n\nplt.scatter(articleDF['title'], articleDF['word_count'])\n\n\n\n\n\n\n\n\nAfter importing the matplotlib.pyplot package, I was well equipped to begin creating exploratory visualizations. To make one that compared articles with their word count, I chose to do a scatterplot, but ran into a few obvious issues:\n\nThe titles, being too long, overlapped over one another.\nThe points were all the same color, making it difficult to distinguish from one another.\n\n\nplt.plot(articleDF['title'], articleDF['word_count'], color='black', linewidth=0.5)\nplt.scatter(articleDF['title'], articleDF['word_count'], c=range(len(articleDF)))\n\nplt.xlabel('Article Title')\nplt.ylabel('Word Count')\nplt.title('Word Count of Various Climate Articles from The Atlantic')\n\nplt.xticks(rotation=45, ha='right')\n\n([0, 1, 2, 3, 4],\n [Text(0, 0, 'Plastic Has Changed Sea Turtles Forever'),\n  Text(1, 0, 'Do Not Allow Even One Fruit Fly Into Your Kitchen Compost'),\n  Text(2, 0, 'One Huge Contradiction Is Undoing Our Best Climate Efforts'),\n  Text(3, 0, 'The Climate Can’t Afford Another Trump Presidency'),\n  Text(4, 0, 'Texas Is a Look Into the Future of Driving')])\n\n\n\n\n\n\n\n\n\nBy adding a color setting within the plt.scatter() function, I was able to create distinctive colors per point, and by rotating the x-axis labels 45 degrees to the right, each title was able to be read by itself without any overlap. After I added a title and labels to the x-axis and y-axis, to make the visual even more clear, I overlapped the scatterplot with a lineplot to accentuate the trend.\nHowever, even with these adjustments, I was still not satisfied with the visualization, and felt that there was a better plot I could choose.\n\nplt.bar(articleDF['title'], articleDF['word_count'])\n\nplt.xlabel('Article Title')\nplt.ylabel('Word Count')\nplt.title('Word Count of Various Climate Articles from The Atlantic')\n\nplt.xticks(rotation=45, ha='right')\n\n([0, 1, 2, 3, 4],\n [Text(0, 0, 'Plastic Has Changed Sea Turtles Forever'),\n  Text(1, 0, 'Do Not Allow Even One Fruit Fly Into Your Kitchen Compost'),\n  Text(2, 0, 'One Huge Contradiction Is Undoing Our Best Climate Efforts'),\n  Text(3, 0, 'The Climate Can’t Afford Another Trump Presidency'),\n  Text(4, 0, 'Texas Is a Look Into the Future of Driving')])\n\n\n\n\n\n\n\n\n\nAfter using those same adjustments on a verticle bar plot, I think this visualization best shows the trend in word count in an understandable and simple way, and gives me a good foundation for the next step in my project.\n\n\n\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment import vader\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import opinion_lexicon\nfrom nltk.stem.porter import PorterStemmer\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('vader_lexicon')\nnltk.download('opinion_lexicon')\n\n[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package opinion_lexicon to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Package opinion_lexicon is already up-to-date!\n\n\nTrue\n\n\nTo begin the data analysis portion of my final project, I have imported the Natural Language Toolkit (NLTK) and its various modules to help with starting a more focused analysis.\n\nstop_words = (stopwords.words('english'))\nsenti = vader.SentimentIntensityAnalyzer()\n\ndef processText(text):\n  tokenized_text = word_tokenize(text)\n  noStop_text = [word for word in tokenized_text if word.lower() not in stop_words]\n  \n  stemmed_text = [PorterStemmer().stem(word) for word in noStop_text]\n  return stemmed_text\n\narticleDF['processed'] = articleDF['text'].apply(processText)\n\nAbove, I began by creating two variables to help me with syntax in using NLTK modules. I then defined a function to process the raw text from The Atlantic articles by tokenizing the string, removing stop words, and words with the same stem.\n\n\n\n\n# compares total frequency of positive and negative words\ndef PosNeg(text):\n  pos_count = 0\n  neg_count = 0\n\n  for word in text:\n    score = senti.polarity_scores(word)\n    if score['compound'] &gt; 0:\n      pos_count += 1\n    elif score['compound'] &lt; 0:\n      neg_count += 1\n\n  return pos_count, neg_count\n\n\nfig, axis = plt.subplots(2, 3, figsize=(18, 12))  # fits all plots in one page\naxis = axis.flatten()\n\nfor i in range(len(articleDF)):   # for each row/article in articleDF\n    row = articleDF.iloc[i]\n\n    pos_count, neg_count = PosNeg(row['processed'])\n\n    axis[i].bar('Positive Words', pos_count, color='yellowgreen', label='Positive Words')\n    axis[i].bar('Negative Words', neg_count, color='firebrick', label='Negative Words')\n\n    axis[i].set_title(row['title'])\n    axis[i].set_xlabel('Word Type')\n    axis[i].set_ylabel('Frequency')\n    axis[i].legend()\n    \nplt.tight_layout()  # no axis overlap\n\n\n\n\n\n\n\n\nThe PosNeg() function takes in a given text and calculates the total number of positive and negative words by using the VADER sentiment analyzer to score each word, incrementing variables pos_count for positive scores and neg_count for negative scores. The function returns the counts of positive and negative words, visualizing information into the general sentiment of the text.\n\n# compares 15 most common positive and negative words\ndef PosNegFreq(text):\n  pos_words = []\n  neg_words = []\n  \n  for word in text:\n    score = senti.polarity_scores(word)\n    if score['compound'] &gt; 0:\n      pos_words.append(word)\n    elif score['compound'] &lt; 0:\n      neg_words.append(word)\n\n  pos_common = nltk.FreqDist(pos_words).most_common(15)\n  neg_common = nltk.FreqDist(neg_words).most_common(15)\n  \n  pos = []\n  pos_freq = []\n  for i in pos_common:\n    pos.append(i[0])\n    pos_freq.append(i[1])\n    \n  neg = []\n  neg_freq = []\n  for i in neg_common:\n    neg.append(i[0])\n    neg_freq.append(i[1])\n    \n  return pos, pos_freq, neg, neg_freq\n\n\nfig, axis = plt.subplots(2, 3, figsize=(18, 12))  # fits all plots in one page\naxis = axis.flatten()\n\nfor i in range(len(articleDF)):   # for each row/article in articleDF\n    row = articleDF.iloc[i]\n\n    pos, pos_freq, neg, neg_freq = PosNegFreq(row['processed'])\n\n    axis[i].barh(neg, neg_freq, color='firebrick', label='Negative Words')\n    axis[i].barh(pos, pos_freq, color='yellowgreen', label='Positive Words')\n\n    axis[i].set_title(row['title'])\n    axis[i].set_xlabel('Frequency')\n    axis[i].set_ylabel('Words')\n    axis[i].legend()\n    \nplt.tight_layout()  # no axis overlap\n\n\n\n\n\n\n\n\nThe PosNegFreq() function identifies the 15 most common positive and negative words in a given text. It classifies words based on their sentiment scores using the VADER analyzer, and then calculates their frequencies, returning four lists: the most common positive words, their frequencies, the most common negative words, and their frequencies.\n\ndef RankByNeg(text):\n  neg_score = []\n  \n  for index, row in text.iterrows():\n    pos_count, neg_count = PosNeg(row['processed'])\n    neg_score.append((row['title'], neg_count))\n    \n  for i in range(len(neg_score)):\n    for j in range(i + 1, len(neg_score)):\n      if neg_score[i][1] &lt; neg_score[j][1]:\n        neg_score[i], neg_score[j] = neg_score[j], neg_score[i]\n  \n  return neg_score\n\n\nneg_scores = RankByNeg(articleDF)\n\ntitles = [item[0] for item in neg_scores]\nscores = [item[1] for item in neg_scores]\n\nplt.barh(titles, scores, color='firebrick')\nplt.xlabel('Negativity Score')\nplt.ylabel('Article Title')\nplt.title('Negative Word Count by Article')\n\nText(0.5, 1.0, 'Negative Word Count by Article')\n\n\n\n\n\n\n\n\n\nThe RankByNeg() function calculates negativity scores (total number of negative words) for each article using the previous PosNeg() function and sorts the list of scores in order.\n\ndef RankByPos(text):\n  pos_score = []\n  \n  for index, row in text.iterrows():\n    pos_count, neg_count = PosNeg(row['processed'])\n    pos_score.append((row['title'], pos_count))\n    \n  for i in range(len(pos_score)):\n    for j in range(i + 1, len(pos_score)):\n      if pos_score[i][1] &lt; pos_score[j][1]:\n        pos_score[i], pos_score[j] = pos_score[j], pos_score[i]\n  \n  return pos_score\n\n\npos_scores = RankByPos(articleDF)\n\ntitles = [item[0] for item in pos_scores]\nscores = [item[1] for item in pos_scores]\n\nplt.barh(titles, scores, color='yellowgreen')\nplt.xlabel('Positivity Score')\nplt.ylabel('Article Title')\nplt.title('Positive Word Count by Article')\n\nText(0.5, 1.0, 'Positive Word Count by Article')\n\n\n\n\n\n\n\n\n\nThe RankByPos() function calculates positivity scores (total number of positive words) for each article using the previous PosNeg() function and sorts the list of scores in order.\n\n\n\nMy preliminary analysis has given me valuable information on how rhetorical strategies are used in written works about climate change. By examining the frequency and sentiment of specific word choices, I have began to uncover patterns that reveal how language is used to frame climate change issues in various articles.\nThe visualizations generated from the data reveal a sharp divide in language use between articles that focus on raising awareness about climate change and those proposing solutions. The article We’re in an Age of Fire exhibits a higher frequency of negative language, with terms such as “fire” and “problem” predominating. This suggests that these articles are more likely to emphasize the urgency and severity of climate change, potentially to provoke concern and action.\nOn the other hand, articles that propose solutions or discuss positive developments, such as A Radical Idea to Break the Logic of Oil Drilling, tend to use more positive language. Words like “warm” and “better”, with higher positive scores, are more frequently used, showing a focus on hopeful narratives and potential advancements. This difference in language use is an example of a rhetorical strategy where negative language is used to show the critical nature of the problem, while positive language is used to convey optimism and the possibility for progress.\nIn regards to my research focus, propaganda typically relies on emotionally charged negative language, while scientific information balance both positive and negative language. With a more robust analysis involving multiple sources and a larger dataset, I could fully answer my research question and differentiate between the two.\n\n\n\nThis research shows the important role of how we talk about climate change. The words and phrases used in climate change articles can strongly influence how people understand and respond to the issue. When the language used shows urgency and the seriousness of the problem, it can encourage people to take action and better understand the crisis. However, if the language downplays the severity or misrepresents the facts, it can slow down efforts to address climate change. Thus, the way climate change is talked about in public can either help or hinder the efforts to deal with this global problem.\nOur findings provide useful information for creating better ways to communicate about climate change, guiding media practices, and supporting efforts to encourage more action and possible policy changes."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]