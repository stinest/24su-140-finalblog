[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "24su-140-finalblog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Climate Talk\n\n\n\n\n\nExploring how rhetoric shapes public views on climate change.\n\n\n\n\n\nAug 1, 2024\n\n\nJustine Lim\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJul 29, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/DH140_JL_Final.html",
    "href": "posts/DH140_JL_Final.html",
    "title": "Analyzing Climate Talk",
    "section": "",
    "text": "In the complex fight against climate change, effective communication is key. Understanding how different rhetoric is used to shape public perception and inspire action can make all the difference. I want to find which types of language are most indicative of propaganda versus scientifically backed information to gauge their influence on public understanding of climate change.\nFrom understanding the ways propaganda and scientifically backed works communicate their messages, we can learn how to better inform and engage the public. Analyzing the language used in these different types of rhetoric will help us understand the divides that influence how climate change is perceived and acted upon. My research intends to break these divisions and instead promote a more informed and unified approach to understanding environmental issues.\nBy not only addressing an urgent global challenge but also using the power of digital humanities to find the intricate ways in which language shapes our world, my study aims to contribute to more effective climate communication strategies for greater public awareness and action.\n\n\n\n\nIn what ways have rhetorical strategies been used in written works about climate change, and what can exploratory data analysis of sentimental language reveal about the presence of propaganda versus scientifically backed information in reputable publications?\n\nAll my data will be compiled from the online Climate section of The Atlantic, because after testing many reputable publications, I found that it has fewer restrictions on web scraping. I will use Python packages specializing in web scraping, such as Beautiful Soup and Requests, to parse HTML documents and extract links to various articles. Once I have access to these articles, I will use Pandas to organize the data into an analyzable format. For text processing, I will utilize NLTK, and for creating visualizations, I will mainly use Matplotlib.\n\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nThe first step in our analysis was to set up the web scraping process. To retrieve data from the web, I imported two libraries, requests to retrieve the HTML code from The Atlantic’s Climate webpage and BeautifulSoup to parse the HTML and find links to various articles on the page.\n\nresponse = requests.get('https://www.theatlantic.com/category/climate/')\nhtml_string = response.text\n\ndoc = BeautifulSoup(html_string, 'html.parser')\narticle_list = []\n    \nfor li in doc.select('ul li a'):\n        href = li.get('href')\n        if href and '/archive' in href:\n            article_list.append(href)\n          \nfor a in doc.find_all('a', href=True):\n    href = a['href']\n    if '/archive' in href:\n        article_list.append(href)\n\nBy searching for all occurrences of anchor tags within lists and also all anchor tags on the page, I created a variable article_list that contains a list of links to the latest Atlantic articles on climate. To avoid irrelevant links to images and other navigation, I filtered specifically for links that contain /archive in their href attribute.\n\ndef getArticleDetails(article):\n  response = requests.get(article)\n  doc = BeautifulSoup(response.text, 'html.parser')\n  \n  title = doc.find('h1')\n  article_title = title.get_text()\n  \n  article_body = doc.find_all(class_='ArticleParagraph_root__4mszW')\n  article_text = ''\n  \n  for paragraph in article_body:\n    article_text += paragraph.get_text() + '\\n'\n    \n  return article_title, article_text\n\nI have defined a function getArticleDetails to open a particular url and return its title and paragraph text in a string form without unnecessary headers or links.\n\nimport random\n\nsampled_articles = random.sample(article_list, 6)\n\nBecause The Atlantic has an abundance of written works, I imported Python’s random module to take a random sample of 6 article URLs from the list to control my data analysis.\n\nimport pandas as pd\n\narticleDF = pd.DataFrame(columns=['url', 'text'])\n\ndf_list = []\n\nfor article_url in sampled_articles:\n  if article_url.startswith('/'):\n    article_url = 'https://www.theatlantic.com' + article_url\n  article_title, article_text = getArticleDetails(article_url)\n    \n  word_count = len(article_text.split())\n  \n  df_list.append(pd.DataFrame({'url': [article_url], 'title': [article_title], 'text': [article_text], 'word_count': [word_count]}))\n\narticleDF = pd.concat(df_list, ignore_index=True)\n\nI have now imported the pandas library to set up for data manipulation and analysis. I then set up a for-loop that retrieved the title and text using the getArticleDetails function defined earlier. I also used the len() tool to calculate each article’s word count. To organize all the data and prevent overwriting variables with each iteration, I created a list to store individual data frames for each article’s article_url, article_title, article_text, and word_count. After collecting all the data frames, I concatenated them into a single data frame to store the information for all articles in the sample.\n\n\n\n\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment import vader\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import opinion_lexicon\nfrom nltk.stem.porter import PorterStemmer\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('vader_lexicon')\nnltk.download('opinion_lexicon')\n\n[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package opinion_lexicon to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Package opinion_lexicon is already up-to-date!\n\n\nTrue\n\n\nTo begin the data analysis portion of my final project, I have imported Matplotlib for visualizations and the Natural Language Toolkit (NLTK) with its various modules to help with starting a more focused analysis.\n\nstop_words = (stopwords.words('english'))\nsenti = vader.SentimentIntensityAnalyzer()\n\ndef processText(text):\n  tokenized_text = word_tokenize(text)\n  noStop_text = [word for word in tokenized_text if word.lower() not in stop_words]\n  \n  stemmed_text = [PorterStemmer().stem(word) for word in noStop_text]\n  return stemmed_text\n\narticleDF['processed'] = articleDF['text'].apply(processText)\n\nAbove, I began by creating two variables to help me with syntax in using NLTK modules. I then defined a function to process the raw text from The Atlantic articles by tokenizing the string, removing stop words, and words with the same stem.\n\n\n\n\n# compares total frequency of positive and negative words\ndef PosNeg(text):\n  pos_count = 0\n  neg_count = 0\n\n  for word in text:\n    score = senti.polarity_scores(word)\n    if score['compound'] &gt; 0:\n      pos_count += 1\n    elif score['compound'] &lt; 0:\n      neg_count += 1\n\n  return pos_count, neg_count\n\n\nfig, axis = plt.subplots(2, 3, figsize=(18, 12))  # fits all plots in one page\naxis = axis.flatten()\n\nfor i in range(len(articleDF)):   # for each row/article in articleDF\n    row = articleDF.iloc[i]\n\n    pos_count, neg_count = PosNeg(row['processed'])\n\n    axis[i].bar('Positive Words', pos_count, color='yellowgreen', label='Positive Words')\n    axis[i].bar('Negative Words', neg_count, color='firebrick', label='Negative Words')\n\n    axis[i].set_title(row['title'])\n    axis[i].set_xlabel('Word Type')\n    axis[i].set_ylabel('Frequency')\n    axis[i].legend()\n    \nplt.tight_layout()  # no axis overlap\n\n\n\n\n\n\n\n\nThe PosNeg() function takes in a given text and calculates the total number of positive and negative words by using the VADER sentiment analyzer to score each word, incrementing variables pos_count for positive scores and neg_count for negative scores. The function returns the counts of positive and negative words, visualizing information into the general sentiment of the text.\n\n# compares 15 most common positive and negative words\ndef PosNegFreq(text):\n  pos_words = []\n  neg_words = []\n  \n  for word in text:\n    score = senti.polarity_scores(word)\n    if score['compound'] &gt; 0:\n      pos_words.append(word)\n    elif score['compound'] &lt; 0:\n      neg_words.append(word)\n\n  pos_common = nltk.FreqDist(pos_words).most_common(15)\n  neg_common = nltk.FreqDist(neg_words).most_common(15)\n  \n  pos = []\n  pos_freq = []\n  for i in pos_common:\n    pos.append(i[0])\n    pos_freq.append(i[1])\n    \n  neg = []\n  neg_freq = []\n  for i in neg_common:\n    neg.append(i[0])\n    neg_freq.append(i[1])\n    \n  return pos, pos_freq, neg, neg_freq\n\n\nfig, axis = plt.subplots(2, 3, figsize=(18, 12))  # fits all plots in one page\naxis = axis.flatten()\n\nfor i in range(len(articleDF)):   # for each row/article in articleDF\n    row = articleDF.iloc[i]\n\n    pos, pos_freq, neg, neg_freq = PosNegFreq(row['processed'])\n\n    axis[i].barh(neg, neg_freq, color='firebrick', label='Negative Words')\n    axis[i].barh(pos, pos_freq, color='yellowgreen', label='Positive Words')\n\n    axis[i].set_title(row['title'])\n    axis[i].set_xlabel('Frequency')\n    axis[i].set_ylabel('Words')\n    axis[i].legend()\n    \nplt.tight_layout()  # no axis overlap\n\n\n\n\n\n\n\n\nThe PosNegFreq() function identifies the 15 most common positive and negative words in a given text. It classifies words based on their sentiment scores using the VADER analyzer, and then calculates their frequencies, returning four lists: the most common positive words, their frequencies, the most common negative words, and their frequencies.\n\ndef RankByNeg(text):\n  neg_score = []\n  \n  for index, row in text.iterrows():\n    pos_count, neg_count = PosNeg(row['processed'])\n    neg_score.append((row['title'], neg_count))\n    \n  for i in range(len(neg_score)):\n    for j in range(i + 1, len(neg_score)):\n      if neg_score[i][1] &lt; neg_score[j][1]:\n        neg_score[i], neg_score[j] = neg_score[j], neg_score[i]\n  \n  return neg_score\n\n\nneg_scores = RankByNeg(articleDF)\n\ntitles = [item[0] for item in neg_scores]\nscores = [item[1] for item in neg_scores]\n\nplt.barh(titles, scores, color='firebrick')\nplt.xlabel('Negativity Score')\nplt.ylabel('Article Title')\nplt.title('Negative Word Count by Article')\n\nText(0.5, 1.0, 'Negative Word Count by Article')\n\n\n\n\n\n\n\n\n\nThe RankByNeg() function calculates negativity scores (total number of negative words) for each article using the previous PosNeg() function and sorts the list of scores in order.\n\ndef RankByPos(text):\n  pos_score = []\n  \n  for index, row in text.iterrows():\n    pos_count, neg_count = PosNeg(row['processed'])\n    pos_score.append((row['title'], pos_count))\n    \n  for i in range(len(pos_score)):\n    for j in range(i + 1, len(pos_score)):\n      if pos_score[i][1] &lt; pos_score[j][1]:\n        pos_score[i], pos_score[j] = pos_score[j], pos_score[i]\n  \n  return pos_score\n\n\npos_scores = RankByPos(articleDF)\n\ntitles = [item[0] for item in pos_scores]\nscores = [item[1] for item in pos_scores]\n\nplt.barh(titles, scores, color='yellowgreen')\nplt.xlabel('Positivity Score')\nplt.ylabel('Article Title')\nplt.title('Positive Word Count by Article')\n\nText(0.5, 1.0, 'Positive Word Count by Article')\n\n\n\n\n\n\n\n\n\nThe RankByPos() function calculates positivity scores (total number of positive words) for each article using the previous PosNeg() function and sorts the list of scores in order.\n\n\n\nMy preliminary analysis has given me valuable information on how rhetorical strategies are used in written works about climate change. By examining the frequency and sentiment of specific word choices, I have began to uncover patterns that reveal how language is used to frame climate change issues in various articles.\nThe visualizations generated from the data reveal a sharp divide in language use between articles that focus on raising awareness about climate change and those proposing solutions. The article We’re in an Age of Fire exhibits a higher frequency of negative language, with terms such as “fire” and “problem” predominating. This suggests that these articles are more likely to emphasize the urgency and severity of climate change, potentially to provoke concern and action.\nOn the other hand, articles that propose solutions or discuss positive developments, such as A Radical Idea to Break the Logic of Oil Drilling, tend to use more positive language. Words like “warm” and “better”, with higher positive scores, are more frequently used, showing a focus on hopeful narratives and potential advancements. This difference in language use is an example of a rhetorical strategy where negative language is used to show the critical nature of the problem, while positive language is used to convey optimism and the possibility for progress.\nIn regards to my research focus, propaganda typically relies on emotionally charged negative language, while scientific information balance both positive and negative language. With a more robust analysis involving multiple sources and a larger dataset, I could fully answer my research question and differentiate between the two.\n\n\n\nThis research shows the important role of how we talk about climate change. The words and phrases used in climate change articles can strongly influence how people understand and respond to the issue. When the language used shows urgency and the seriousness of the problem, it can encourage people to take action and better understand the crisis. However, if the language downplays the severity or misrepresents the facts, it can slow down efforts to address climate change. Thus, the way climate change is talked about in public can either help or hinder the efforts to deal with this global problem.\nOur findings provide useful information for creating better ways to communicate about climate change, guiding media practices, and supporting efforts to encourage more action and possible policy changes."
  },
  {
    "objectID": "posts/DH140_JL_Final.html#dh140-final-project-by-justine-lim",
    "href": "posts/DH140_JL_Final.html#dh140-final-project-by-justine-lim",
    "title": "Analyzing Climate Talk",
    "section": "",
    "text": "In the complex fight against climate change, effective communication is key. Understanding how different rhetoric is used to shape public perception and inspire action can make all the difference. I want to find which types of language are most indicative of propaganda versus scientifically backed information to gauge their influence on public understanding of climate change.\nFrom understanding the ways propaganda and scientifically backed works communicate their messages, we can learn how to better inform and engage the public. Analyzing the language used in these different types of rhetoric will help us understand the divides that influence how climate change is perceived and acted upon. My research intends to break these divisions and instead promote a more informed and unified approach to understanding environmental issues.\nBy not only addressing an urgent global challenge but also using the power of digital humanities to find the intricate ways in which language shapes our world, my study aims to contribute to more effective climate communication strategies for greater public awareness and action.\n\n\n\n\nIn what ways have rhetorical strategies been used in written works about climate change, and what can exploratory data analysis of sentimental language reveal about the presence of propaganda versus scientifically backed information in reputable publications?\n\nAll my data will be compiled from the online Climate section of The Atlantic, because after testing many reputable publications, I found that it has fewer restrictions on web scraping. I will use Python packages specializing in web scraping, such as Beautiful Soup and Requests, to parse HTML documents and extract links to various articles. Once I have access to these articles, I will use Pandas to organize the data into an analyzable format. For text processing, I will utilize NLTK, and for creating visualizations, I will mainly use Matplotlib.\n\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nThe first step in our analysis was to set up the web scraping process. To retrieve data from the web, I imported two libraries, requests to retrieve the HTML code from The Atlantic’s Climate webpage and BeautifulSoup to parse the HTML and find links to various articles on the page.\n\nresponse = requests.get('https://www.theatlantic.com/category/climate/')\nhtml_string = response.text\n\ndoc = BeautifulSoup(html_string, 'html.parser')\narticle_list = []\n    \nfor li in doc.select('ul li a'):\n        href = li.get('href')\n        if href and '/archive' in href:\n            article_list.append(href)\n          \nfor a in doc.find_all('a', href=True):\n    href = a['href']\n    if '/archive' in href:\n        article_list.append(href)\n\nBy searching for all occurrences of anchor tags within lists and also all anchor tags on the page, I created a variable article_list that contains a list of links to the latest Atlantic articles on climate. To avoid irrelevant links to images and other navigation, I filtered specifically for links that contain /archive in their href attribute.\n\ndef getArticleDetails(article):\n  response = requests.get(article)\n  doc = BeautifulSoup(response.text, 'html.parser')\n  \n  title = doc.find('h1')\n  article_title = title.get_text()\n  \n  article_body = doc.find_all(class_='ArticleParagraph_root__4mszW')\n  article_text = ''\n  \n  for paragraph in article_body:\n    article_text += paragraph.get_text() + '\\n'\n    \n  return article_title, article_text\n\nI have defined a function getArticleDetails to open a particular url and return its title and paragraph text in a string form without unnecessary headers or links.\n\nimport random\n\nsampled_articles = random.sample(article_list, 6)\n\nBecause The Atlantic has an abundance of written works, I imported Python’s random module to take a random sample of 6 article URLs from the list to control my data analysis.\n\nimport pandas as pd\n\narticleDF = pd.DataFrame(columns=['url', 'text'])\n\ndf_list = []\n\nfor article_url in sampled_articles:\n  if article_url.startswith('/'):\n    article_url = 'https://www.theatlantic.com' + article_url\n  article_title, article_text = getArticleDetails(article_url)\n    \n  word_count = len(article_text.split())\n  \n  df_list.append(pd.DataFrame({'url': [article_url], 'title': [article_title], 'text': [article_text], 'word_count': [word_count]}))\n\narticleDF = pd.concat(df_list, ignore_index=True)\n\nI have now imported the pandas library to set up for data manipulation and analysis. I then set up a for-loop that retrieved the title and text using the getArticleDetails function defined earlier. I also used the len() tool to calculate each article’s word count. To organize all the data and prevent overwriting variables with each iteration, I created a list to store individual data frames for each article’s article_url, article_title, article_text, and word_count. After collecting all the data frames, I concatenated them into a single data frame to store the information for all articles in the sample.\n\n\n\n\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment import vader\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import opinion_lexicon\nfrom nltk.stem.porter import PorterStemmer\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('vader_lexicon')\nnltk.download('opinion_lexicon')\n\n[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package opinion_lexicon to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Package opinion_lexicon is already up-to-date!\n\n\nTrue\n\n\nTo begin the data analysis portion of my final project, I have imported Matplotlib for visualizations and the Natural Language Toolkit (NLTK) with its various modules to help with starting a more focused analysis.\n\nstop_words = (stopwords.words('english'))\nsenti = vader.SentimentIntensityAnalyzer()\n\ndef processText(text):\n  tokenized_text = word_tokenize(text)\n  noStop_text = [word for word in tokenized_text if word.lower() not in stop_words]\n  \n  stemmed_text = [PorterStemmer().stem(word) for word in noStop_text]\n  return stemmed_text\n\narticleDF['processed'] = articleDF['text'].apply(processText)\n\nAbove, I began by creating two variables to help me with syntax in using NLTK modules. I then defined a function to process the raw text from The Atlantic articles by tokenizing the string, removing stop words, and words with the same stem.\n\n\n\n\n# compares total frequency of positive and negative words\ndef PosNeg(text):\n  pos_count = 0\n  neg_count = 0\n\n  for word in text:\n    score = senti.polarity_scores(word)\n    if score['compound'] &gt; 0:\n      pos_count += 1\n    elif score['compound'] &lt; 0:\n      neg_count += 1\n\n  return pos_count, neg_count\n\n\nfig, axis = plt.subplots(2, 3, figsize=(18, 12))  # fits all plots in one page\naxis = axis.flatten()\n\nfor i in range(len(articleDF)):   # for each row/article in articleDF\n    row = articleDF.iloc[i]\n\n    pos_count, neg_count = PosNeg(row['processed'])\n\n    axis[i].bar('Positive Words', pos_count, color='yellowgreen', label='Positive Words')\n    axis[i].bar('Negative Words', neg_count, color='firebrick', label='Negative Words')\n\n    axis[i].set_title(row['title'])\n    axis[i].set_xlabel('Word Type')\n    axis[i].set_ylabel('Frequency')\n    axis[i].legend()\n    \nplt.tight_layout()  # no axis overlap\n\n\n\n\n\n\n\n\nThe PosNeg() function takes in a given text and calculates the total number of positive and negative words by using the VADER sentiment analyzer to score each word, incrementing variables pos_count for positive scores and neg_count for negative scores. The function returns the counts of positive and negative words, visualizing information into the general sentiment of the text.\n\n# compares 15 most common positive and negative words\ndef PosNegFreq(text):\n  pos_words = []\n  neg_words = []\n  \n  for word in text:\n    score = senti.polarity_scores(word)\n    if score['compound'] &gt; 0:\n      pos_words.append(word)\n    elif score['compound'] &lt; 0:\n      neg_words.append(word)\n\n  pos_common = nltk.FreqDist(pos_words).most_common(15)\n  neg_common = nltk.FreqDist(neg_words).most_common(15)\n  \n  pos = []\n  pos_freq = []\n  for i in pos_common:\n    pos.append(i[0])\n    pos_freq.append(i[1])\n    \n  neg = []\n  neg_freq = []\n  for i in neg_common:\n    neg.append(i[0])\n    neg_freq.append(i[1])\n    \n  return pos, pos_freq, neg, neg_freq\n\n\nfig, axis = plt.subplots(2, 3, figsize=(18, 12))  # fits all plots in one page\naxis = axis.flatten()\n\nfor i in range(len(articleDF)):   # for each row/article in articleDF\n    row = articleDF.iloc[i]\n\n    pos, pos_freq, neg, neg_freq = PosNegFreq(row['processed'])\n\n    axis[i].barh(neg, neg_freq, color='firebrick', label='Negative Words')\n    axis[i].barh(pos, pos_freq, color='yellowgreen', label='Positive Words')\n\n    axis[i].set_title(row['title'])\n    axis[i].set_xlabel('Frequency')\n    axis[i].set_ylabel('Words')\n    axis[i].legend()\n    \nplt.tight_layout()  # no axis overlap\n\n\n\n\n\n\n\n\nThe PosNegFreq() function identifies the 15 most common positive and negative words in a given text. It classifies words based on their sentiment scores using the VADER analyzer, and then calculates their frequencies, returning four lists: the most common positive words, their frequencies, the most common negative words, and their frequencies.\n\ndef RankByNeg(text):\n  neg_score = []\n  \n  for index, row in text.iterrows():\n    pos_count, neg_count = PosNeg(row['processed'])\n    neg_score.append((row['title'], neg_count))\n    \n  for i in range(len(neg_score)):\n    for j in range(i + 1, len(neg_score)):\n      if neg_score[i][1] &lt; neg_score[j][1]:\n        neg_score[i], neg_score[j] = neg_score[j], neg_score[i]\n  \n  return neg_score\n\n\nneg_scores = RankByNeg(articleDF)\n\ntitles = [item[0] for item in neg_scores]\nscores = [item[1] for item in neg_scores]\n\nplt.barh(titles, scores, color='firebrick')\nplt.xlabel('Negativity Score')\nplt.ylabel('Article Title')\nplt.title('Negative Word Count by Article')\n\nText(0.5, 1.0, 'Negative Word Count by Article')\n\n\n\n\n\n\n\n\n\nThe RankByNeg() function calculates negativity scores (total number of negative words) for each article using the previous PosNeg() function and sorts the list of scores in order.\n\ndef RankByPos(text):\n  pos_score = []\n  \n  for index, row in text.iterrows():\n    pos_count, neg_count = PosNeg(row['processed'])\n    pos_score.append((row['title'], pos_count))\n    \n  for i in range(len(pos_score)):\n    for j in range(i + 1, len(pos_score)):\n      if pos_score[i][1] &lt; pos_score[j][1]:\n        pos_score[i], pos_score[j] = pos_score[j], pos_score[i]\n  \n  return pos_score\n\n\npos_scores = RankByPos(articleDF)\n\ntitles = [item[0] for item in pos_scores]\nscores = [item[1] for item in pos_scores]\n\nplt.barh(titles, scores, color='yellowgreen')\nplt.xlabel('Positivity Score')\nplt.ylabel('Article Title')\nplt.title('Positive Word Count by Article')\n\nText(0.5, 1.0, 'Positive Word Count by Article')\n\n\n\n\n\n\n\n\n\nThe RankByPos() function calculates positivity scores (total number of positive words) for each article using the previous PosNeg() function and sorts the list of scores in order.\n\n\n\nMy preliminary analysis has given me valuable information on how rhetorical strategies are used in written works about climate change. By examining the frequency and sentiment of specific word choices, I have began to uncover patterns that reveal how language is used to frame climate change issues in various articles.\nThe visualizations generated from the data reveal a sharp divide in language use between articles that focus on raising awareness about climate change and those proposing solutions. The article We’re in an Age of Fire exhibits a higher frequency of negative language, with terms such as “fire” and “problem” predominating. This suggests that these articles are more likely to emphasize the urgency and severity of climate change, potentially to provoke concern and action.\nOn the other hand, articles that propose solutions or discuss positive developments, such as A Radical Idea to Break the Logic of Oil Drilling, tend to use more positive language. Words like “warm” and “better”, with higher positive scores, are more frequently used, showing a focus on hopeful narratives and potential advancements. This difference in language use is an example of a rhetorical strategy where negative language is used to show the critical nature of the problem, while positive language is used to convey optimism and the possibility for progress.\nIn regards to my research focus, propaganda typically relies on emotionally charged negative language, while scientific information balance both positive and negative language. With a more robust analysis involving multiple sources and a larger dataset, I could fully answer my research question and differentiate between the two.\n\n\n\nThis research shows the important role of how we talk about climate change. The words and phrases used in climate change articles can strongly influence how people understand and respond to the issue. When the language used shows urgency and the seriousness of the problem, it can encourage people to take action and better understand the crisis. However, if the language downplays the severity or misrepresents the facts, it can slow down efforts to address climate change. Thus, the way climate change is talked about in public can either help or hinder the efforts to deal with this global problem.\nOur findings provide useful information for creating better ways to communicate about climate change, guiding media practices, and supporting efforts to encourage more action and possible policy changes."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]